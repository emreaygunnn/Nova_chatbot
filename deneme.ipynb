{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3354473",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.read_csv(r\"C:\\Users\\EMRE\\Desktop\\nlp_proje\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b153f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse(convo_str):\n",
    "    try:\n",
    "     \n",
    "     return json.loads(convo_str.replace(\"'\", '\"'))\n",
    "    except Exception as e:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14184e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"parsed_conversations\"] = df[\"conversations\"].apply(safe_parse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b697b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = df[df[\"parsed_conversations\"].notnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34347a",
   "metadata": {},
   "outputs": [],
   "source": [
    " prompts = []\n",
    "responses = []\n",
    "\n",
    "for convo in df[\"parsed_conversations\"]:\n",
    "  for i in range(len(convo) - 1):\n",
    "    if convo[i][\"from\"] == \"human\" and convo[i + 1][\"from\"] == \"gpt\":\n",
    "        prompts.append(convo[i][\"value\"])\n",
    "        responses.append(convo[i + 1][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e617d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    " chat_df = pd.DataFrame({\n",
    "\"prompt\": prompts,\n",
    "\"response\": responses\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d78063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\EMRE\\Desktop\\nlp_proje\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "def parse_conversation(raw_text):\n",
    "    try:\n",
    "# S√∂zl√ºk listesi olarak ayrƒ±≈ütƒ±r\n",
    "     convo = ast.literal_eval(raw_text)\n",
    "     for i in range(len(convo)-1):\n",
    "          if convo[i][\"from\"] == \"human\" and convo[i+1][\"from\"] == \"gpt\":\n",
    "           prompts.append(convo[i][\"value\"])\n",
    "           responses.append(convo[i+1][\"value\"])\n",
    "    except Exception as e:\n",
    "      pass # hatalƒ± satƒ±r varsa atla  sonraki df[\"conversations\"].apply(parse_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"prompt\": prompts, \"response\": responses})\n",
    "print(data.head())\n",
    "print(f\"Toplam {len(data)} adet soru-cevap √ßifti √ßƒ±karƒ±ldƒ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b461aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\EMRE\\Desktop\\nlp_proje\\train.csv\")\n",
    "print(df[\"conversations\"].iloc[0])\n",
    "print()\n",
    "print(type(df[\"conversations\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fe6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\EMRE\\Desktop\\nlp_proje\\train.csv\")\n",
    "def fix_json_string(s):\n",
    "# '}{' aralarƒ±na virg√ºl ekle ‚Üí '}, {'\n",
    " s_fixed = re.sub(r'}\\s*{', '}, {', s.strip())\n",
    " return ast.literal_eval(s_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re # ‚Üê BU SATIR EKLENDƒ∞\n",
    "import ast\n",
    "df[\"conversations_fixed\"] = df[\"conversations\"].apply(fix_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470763b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    " print(f\"--- Satƒ±r {i} ---\")\n",
    "for turn in df[\"conversations_fixed\"].iloc[i]:\n",
    " print(f\"{turn['from']}: {turn['value']}\")\n",
    " print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    " from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd310a",
   "metadata": {},
   "outputs": [],
   "source": [
    " pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv in tqdm(df[\"conversations_fixed\"]):\n",
    " for i in range(len(conv) - 1):\n",
    "  if conv[i][\"from\"] == \"human\" and conv[i + 1][\"from\"] == \"gpt\":\n",
    "   prompt = conv[i][\"value\"].strip()\n",
    "   response = conv[i + 1][\"value\"].strip()\n",
    "   pairs.append((prompt, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535af47b",
   "metadata": {},
   "outputs": [],
   "source": [
    " final_df = pd.DataFrame(pairs, columns=[\"prompt\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"empathy_chat_data.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66e178",
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install faiss-cpu sentence-transformers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f529cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json_string(s):\n",
    " import re\n",
    " s_fixed = re.sub(r'}\\s*{', '}, {', s.strip())\n",
    " return ast.literal_eval(s_fixed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c334088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "     convo = fix_json_string(row[\"conversations\"])\n",
    "     for j in range(len(convo) - 1):\n",
    "      if convo[j][\"from\"] == \"human\" and convo[j+1][\"from\"] == \"gpt\":\n",
    "       prompt = convo[j][\"value\"].strip()\n",
    "       response = convo[j+1][\"value\"].strip()\n",
    "       data.append({\"prompt\": prompt, \"response\": response})\n",
    "    except Exception as e:\n",
    "     continue # bozuk satƒ±rlarƒ± atla\n",
    "\n",
    "df_ready = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b98c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag = df_ready.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3406d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode(df_rag[\"prompt\"].tolist(), convert_to_numpy=True, show_progress_bar=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(corpus_embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"rag_index.faiss\")\n",
    "with open(\"rag_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_rag, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596200ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS index'i ve dataframe'i y√ºkle\n",
    "index = faiss.read_index(\"rag_index.faiss\")\n",
    "with open(\"rag_data.pkl\", \"rb\") as f:\n",
    "    df_rag = pickle.load(f)\n",
    "\n",
    "# Aynƒ± embedding modelini yeniden y√ºkle\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chatbot(user_input, top_k=3):\n",
    "    query_embedding = model.encode([user_input], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    print(\"\\n--- En ƒ∞lgili Cevaplar ---\")\n",
    "    for i in range(top_k):\n",
    "        print(f\"\\n[+] Soru: {df_rag.iloc[indices[0][i]]['prompt']}\")\n",
    "        print(f\"[=] Cevap: {df_rag.iloc[indices[0][i]]['response']}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e01c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chatbot(\"Son zamanlarda kendimi yalnƒ±z ve umutsuz hissediyorum. Ne yapmalƒ±yƒ±m?\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers faiss-cpu pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2be685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Dosyalarƒ± y√ºkle\n",
    "feedback_df = pd.read_csv(\"feedback_data.csv\")\n",
    "\n",
    "with open(\"rag_data.pkl\", \"rb\") as f:\n",
    "    df_rag = pickle.load(f)\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Geri bildirimleri uygula\n",
    "for _, row in feedback_df.iterrows():\n",
    "    prompt = row[\"prompt\"]\n",
    "    original = row[\"original_response\"]\n",
    "    improved = row[\"improved_response\"]\n",
    "\n",
    "    mask = (df_rag[\"prompt\"] == prompt) & (df_rag[\"response\"] == original)\n",
    "    df_rag.loc[mask, \"response\"] = improved\n",
    "\n",
    "# Yeni embedding'leri √ºret\n",
    "embeddings = model.encode(df_rag[\"prompt\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# FAISS index‚Äôi yeniden olu≈ütur\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# G√ºncellenmi≈ü dosyalarƒ± kaydet\n",
    "with open(\"rag_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_rag, f)\n",
    "\n",
    "faiss.write_index(index, \"rag_index.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d55624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from deep_translator import GoogleTranslator\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Embed modeli\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Veri yollarƒ±\n",
    "rag_path = \"rag_data.pkl\"\n",
    "index_path = \"rag_index.faiss\"\n",
    "feedback_path = \"feedback_data.csv\"\n",
    "\n",
    "# Ba≈ülangƒ±√ßta FAISS ve RAG verisi y√ºkleniyor\n",
    "with open(rag_path, \"rb\") as f:\n",
    "    df_rag = pickle.load(f)\n",
    "index = faiss.read_index(index_path)\n",
    "\n",
    "# √áeviri fonksiyonu\n",
    "def translate(text, src='auto', tgt='en'):\n",
    "    try:\n",
    "        return GoogleTranslator(source=src, target=tgt).translate(text)\n",
    "    except Exception:\n",
    "        return \"[√áeviri Hatasƒ±]\"\n",
    "\n",
    "# ƒ∞sim temizleyici\n",
    "def clean_name(text):\n",
    "    text = re.sub(r\"\\b(Hi|Hello|Hey)\\s+Charlie\\b[:,]?\", \"\", text)\n",
    "    text = re.sub(r\"\\bCharlie\\b[:,]?\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Nova yanƒ±t √ºretici\n",
    "def rag_chatbot(user_input):\n",
    "    translated_input = translate(user_input, src=\"tr\", tgt=\"en\")\n",
    "    query_embedding = model.encode([translated_input])\n",
    "    D, I = index.search(np.array(query_embedding), k=3)\n",
    "    response_en = df_rag.iloc[I[0][0]][\"response\"]\n",
    "    response_en = clean_name(response_en)\n",
    "    response_tr = translate(response_en, src=\"en\", tgt=\"tr\")\n",
    "    return response_tr\n",
    "\n",
    "# Geri bildirim kaydedici\n",
    "def save_feedback(original_prompt, nova_response, improved_response):\n",
    "    feedback_entry = {\n",
    "        \"prompt\": original_prompt,\n",
    "        \"original_response\": nova_response,\n",
    "        \"improved_response\": improved_response\n",
    "    }\n",
    "    if os.path.exists(feedback_path):\n",
    "        df_feedback = pd.read_csv(feedback_path)\n",
    "        df_feedback = pd.concat([df_feedback, pd.DataFrame([feedback_entry])], ignore_index=True)\n",
    "    else:\n",
    "        df_feedback = pd.DataFrame([feedback_entry])\n",
    "    df_feedback.to_csv(feedback_path, index=False)\n",
    "    return \"Geri bildiriminiz kaydedildi. Te≈üekk√ºrler!\"\n",
    "\n",
    "# Geri bildirim sonrasƒ± g√∂r√ºn√ºr mesaj g√∂sterici\n",
    "def save_feedback_and_show_status(prompt, response, improved):\n",
    "    message = save_feedback(prompt, response, improved)\n",
    "    return gr.update(value=message, visible=True)\n",
    "\n",
    "# Geri bildirimlere g√∂re veriyi g√ºncelle (sadece deƒüi≈üen satƒ±rlar embed edilir)\n",
    "def update_rag_data():\n",
    "    try:\n",
    "        if not os.path.exists(feedback_path) or not os.path.exists(rag_path):\n",
    "            return \"Gerekli dosyalar bulunamadƒ±.\"\n",
    "\n",
    "        df_feedback = pd.read_csv(feedback_path)\n",
    "        with open(rag_path, \"rb\") as f:\n",
    "            df_rag_local = pickle.load(f)\n",
    "\n",
    "        updated_prompts = []\n",
    "        for _, row in df_feedback.iterrows():\n",
    "            prompt = row[\"prompt\"]\n",
    "            original = row[\"original_response\"]\n",
    "            improved = row[\"improved_response\"]\n",
    "\n",
    "            mask = (df_rag_local[\"prompt\"] == prompt) & (df_rag_local[\"response\"] == original)\n",
    "            if mask.any():\n",
    "                idx = df_rag_local[mask].index[0]\n",
    "                if df_rag_local.at[idx, \"response\"] != improved:\n",
    "                    df_rag_local.at[idx, \"response\"] = improved\n",
    "                    updated_prompts.append(prompt)\n",
    "            else:\n",
    "                new_row = {\"prompt\": prompt, \"response\": improved}\n",
    "                df_rag_local = pd.concat([df_rag_local, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                updated_prompts.append(prompt)\n",
    "\n",
    "        if updated_prompts:\n",
    "            new_embeddings = model.encode(updated_prompts, show_progress_bar=False)\n",
    "            index.add(np.array(new_embeddings))\n",
    "\n",
    "            # Global veri g√ºncelle\n",
    "            global df_rag\n",
    "            df_rag = df_rag_local\n",
    "\n",
    "            # G√ºncellenmi≈ü dosyalarƒ± kaydet\n",
    "            with open(rag_path, \"wb\") as f:\n",
    "                pickle.dump(df_rag_local, f)\n",
    "            faiss.write_index(index, index_path)\n",
    "\n",
    "            return f\"{len(updated_prompts)} cevap g√ºncellendi ‚úÖ\"\n",
    "        else:\n",
    "            return \"Yeni g√ºncellenecek i√ßerik bulunamadƒ±.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Hata olu≈ütu: {str(e)}\"\n",
    "\n",
    "# Gradio Aray√ºz\n",
    "with gr.Blocks(title=\"Nova - Empatik Chatbot\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ü§ñ Nova  \n",
    "        Kƒ±sa, empatik, T√ºrk√ße yanƒ±tlar sunan bir sohbet arkada≈üƒ±.  \n",
    "        Kendini ifade et, Nova seni dinliyor.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            user_input = gr.Textbox(label=\"Mesajƒ±n\", placeholder=\"Bug√ºn nasƒ±l hissediyorsun?\", lines=4)\n",
    "            submit_btn = gr.Button(\"Nova'ya Sor\")\n",
    "        with gr.Column(scale=5):\n",
    "            chatbot_output = gr.Textbox(label=\"Nova'nƒ±n Cevabƒ±\", lines=5, interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        update_button = gr.Button(\"üîÅ Cevaplarƒ± G√ºncelle\")\n",
    "        update_status = gr.Textbox(label=\"\", interactive=False, visible=True)\n",
    "\n",
    "    # Geri bildirim alanƒ±\n",
    "    with gr.Accordion(\"Nova'nƒ±n cevabƒ±nƒ± geli≈ütirmek ister misin?\", open=False):\n",
    "        improved_reply = gr.Textbox(label=\"Daha iyi bir √∂nerin varsa buraya yaz.\")\n",
    "        submit_feedback_btn = gr.Button(\"Geri Bildirim G√∂nder\")\n",
    "        feedback_status = gr.Textbox(label=\"\", interactive=False, visible=False)\n",
    "\n",
    "    submit_btn.click(fn=rag_chatbot, inputs=user_input, outputs=chatbot_output)\n",
    "    submit_feedback_btn.click(\n",
    "        fn=save_feedback_and_show_status,\n",
    "        inputs=[user_input, chatbot_output, improved_reply],\n",
    "        outputs=feedback_status\n",
    "    )\n",
    "    update_button.click(fn=update_rag_data, inputs=[], outputs=update_status)\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
